<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><link rel="apple-touch-icon" sizes="57x57" href="/favicon-57x57.png"/><link rel="apple-touch-icon" sizes="60x60" href="/favicon-60x60.png"/><link rel="apple-touch-icon" sizes="72x72" href="/favicon-72x72.png"/><link rel="apple-touch-icon" sizes="76x76" href="/favicon-76x76.png"/><link rel="apple-touch-icon" sizes="114x114" href="/favicon-114x114.png"/><link rel="apple-touch-icon" sizes="120x120" href="/favicon-120x120.png"/><link rel="apple-touch-icon" sizes="144x144" href="/favicon-144x144.png"/><link rel="apple-touch-icon" sizes="152x152" href="/favicon-152x152.png"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon-180x180.png"/><link rel="icon" type="image/svg+xml" href="/favicon.svg"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png"/><link rel="icon" type="image/png" sizes="192x192" href="/favicon-192x192.png"/><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"/><link rel="icon" type="image/x-icon" href="/favicon.ico"/><meta name="msapplication-TileImage" content="/favicon-144x144.png"/><meta name="msapplication-config" content="/browserconfig.xml"/><meta name="theme-color" content="#0E151B"/><link rel="alternate" type="application/rss+xml" href="https://reuben.science/rss.xml"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@_reubn"/><meta name="twitter:creator" content="@_reubn"/><meta property="og:locale" content="en_GB"/><title>Making My Interactive Memoji | Reuben</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Interactive Memoji for the Web"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2020-12-16T00:00:00.000Z"/><meta property="article:author" content="Reuben"/><meta property="article:tag" content="dev"/><meta property="og:title" content="Making My Interactive Memoji"/><meta property="og:description" content="Interactive Memoji for the Web"/><meta property="og:image" content="https://reuben.science/.assets/8225697f392f56e9e1b65e4cc963bc3c.webp"/><meta property="og:image:alt" content="Making My Interactive Memoji"/><meta property="og:image:width" content="1000"/><meta property="og:image:height" content="400"/><meta property="og:site_name" content="reuben.science"/><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      {
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https://reuben.science/",
          "name": "Home"
        }
      },{
        "@type": "ListItem",
        "position": 2,
        "item": {
          "@id": "https://reuben.science/posts",
          "name": "Posts"
        }
      },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https://reuben.science/posts/making-my-memoji",
          "name": "Making My Interactive Memoji"
        }
      }
     ]
  }</script><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "undefined"
    },
    "headline": "Making My Interactive Memoji",
    "image": [
      
     ],
    "datePublished": "2020-12-16T00:00:00.000Z",
    "dateModified": "2020-12-16T00:00:00.000Z",
    "author": [{"@type": "Person","name": "Reuben"}],
    "publisher": {
      "@type": "Organization",
      "name": "Reuben",
      "logo": {
        "@type": "ImageObject",
        "url": "undefined"
      }
    },
    "description": "Interactive Memoji for the Web"
  }</script><meta name="next-head-count" content="43"/><link rel="preload" href="/_next/static/css/8b548dd154129270ac4f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8b548dd154129270ac4f.css" data-n-g=""/><link rel="preload" href="/_next/static/css/af6d15af1bd31b994104.css" as="style"/><link rel="stylesheet" href="/_next/static/css/af6d15af1bd31b994104.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-6977ff558a8644f11edb.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-7ec0fa63faecf3764afe.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.28e15cca58e295c8d62e.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.a7eaae1b56f101b95777.js" as="script"/><link rel="preload" href="/_next/static/chunks/75dcd2c344866c2e9d678f03e7caf5a73214c3a2.374f04e66024ab278f54.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-337c0d7807df1f95e2d3.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/posts/%5Bslug%5D-1fb1690444c44a1b1d3d.js" as="script"/></head><body><div id="__next"><section class="_app_app__7eKMU" id="app"><header class="styles_header__1VY7O"><svg class="styles_logo__CS-TL  styles_svg__2WrhB" viewBox="0 0 994 994"><defs><linearGradient x1="0%" y1="0%" x2="99.9884439%" y2="99.9884439%" id="orange"><stop stop-color="hsl(14, 100%, 61%)" offset="0.0115561319%"></stop><stop stop-color="hsl(32, 97%, 57%)" offset="100%"></stop></linearGradient><linearGradient x1="100%" y1="100%" x2="0%" y2="0%" id="yellow"><stop stop-color="hsl(56, 97%, 57%)" offset="0%"></stop><stop stop-color="hsl(51, 100%, 61%)" offset="100%"></stop></linearGradient><linearGradient x1="8.8817842e-14%" y1="8.8817842e-14%" x2="100%" y2="100%" id="red"><stop stop-color="hsl(0, 100%, 66%)" offset="0%"></stop><stop stop-color="hsl(0, 97%, 57%)" offset="100%"></stop></linearGradient><linearGradient x1="0%" y1="0%" x2="100%" y2="100%" id="blue"><stop stop-color="hsl(180, 90%, 48%)" offset="0%"></stop><stop stop-color="hsl(201, 97%, 50%)" offset="100%"></stop></linearGradient><linearGradient x1="0%" y1="-1.77635684e-13%" x2="100%" y2="100%" id="purple"><stop stop-color="hsl(244, 100%, 66%)" offset="0%"></stop><stop stop-color="hsl(274, 97%, 57%)" offset="100%"></stop></linearGradient><linearGradient x1="100%" y1="100%" x2="-7.10542736e-13%" y2="7.10542736e-13%" id="green"><stop stop-color="hsl(117, 100%, 66%)" offset="0%"></stop><stop stop-color="hsl(158, 97%, 57%)" offset="100%"></stop></linearGradient></defs><g><g transform="translate(-565.000000, -233.000000)"><g transform="translate(1062.000000, 730.000000) rotate(-45.000000) translate(-1062.000000, -730.000000) translate(712.000000, 378.000000)"><path d="M400,20.0660172 L400,230.066017 C400,241.111712 391.045695,250.066017 380,250.066017 L270,250.066017 C258.954305,250.066017 250,259.020322 250,270.066017 L250,280.066017 C250,291.111712 258.954305,300.066017 270,300.066017 L680,300.066017 C691.045695,300.066017 700,309.020322 700,320.066017 L700,680.066017 C700,691.111712 691.045695,700.066017 680,700.066017 L320,700.066017 C308.954305,700.066017 300,691.111712 300,680.066017 L300,620.066017 C300,609.020322 308.954305,600.066017 320,600.066017 L580,600.066017 C591.045695,600.066017 600,591.111712 600,580.066017 L600,570.066017 C600,559.020322 591.045695,550.066017 580,550.066017 L320,550.066017 C308.954305,550.066017 300,541.111712 300,530.066017 L300,470.066017 C300,459.020322 308.954305,450.066017 320,450.066017 L580,450.066017 C591.045695,450.066017 600,441.111712 600,430.066017 L600,420.066017 C600,409.020322 591.045695,400.066017 580,400.066017 L170,400.066017 C158.954305,400.066017 150,391.111712 150,380.066017 L150,170.066017 C150,159.020322 158.954305,150.066017 170,150.066017 L280,150.066017 C291.045695,150.066017 300,141.111712 300,130.066017 L300,120.066017 C300,109.020322 291.045695,100.066017 280,100.066017 L120,100.066017 C108.954305,100.066017 100,109.020322 100,120.066017 L100,380.066017 C100,391.111712 91.045695,400.066017 80,400.066017 L20,400.066017 C8.954305,400.066017 1.3527075e-15,391.111712 0,380.066017 L0,20.0660172 C-1.3527075e-15,9.02032218 8.954305,0.066017178 20,0.066017178 L380,0.066017178 C391.045695,0.066017178 400,9.02032218 400,20.0660172 Z" class="styles_re__3Bp-3"></path><g transform="translate(0.000000, 0.066017)"><g transform="translate(449.933983, 0.000000)"><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="0.066017178" y="0" rx="20px"></rect><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="2.84217094e-14" y="149.93" rx="20px"></rect><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="150" y="149.93" rx="20px"></rect><rect class="styles_ex__1cZNS styles_red__2EFtt " fill="url(#red)" x="0.066017178" y="0" rx="20px"></rect><rect class="styles_ex__1cZNS styles_orange__l75CZ " fill="url(#orange)" x="2.84217094e-14" y="149.93" rx="20px"></rect><rect class="styles_ex__1cZNS styles_yellow__-MCXd " fill="url(#yellow)" x="150" y="149.93" rx="20px"></rect></g><g transform="translate(0.000000, 452.933983)"><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="5.68434189e-14" y="0.066017178" rx="20px"></rect><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="150.066017" y="1.13686838e-13" rx="20px"></rect><rect class="styles_ex__1cZNS styles_dim__3uRUr" x="149.933983" y="149.996017" rx="20px"></rect><rect class="styles_ex__1cZNS styles_green__1PJmO " fill="url(#green)" x="5.68434189e-14" y="0.066017178" rx="20px"></rect><rect class="styles_ex__1cZNS styles_blue__qsmnZ " fill="url(#blue)" x="150.066017" y="1.13686838e-13" rx="20px"></rect><rect class="styles_ex__1cZNS styles_purple__2RPlF " fill="url(#purple)" x="149.933983" y="149.996017" rx="20px"></rect></g></g></g></g></g></svg><nav class="styles_nav__92QWc"><a class="styles_link__3mBj1 " style="--link-underline:linear-gradient(135deg, var(--colours-pink-red) 0%, var(--colours-pink-red-hint) 100%)" href="/">Home</a><a class="styles_link__3mBj1 " style="--link-underline:linear-gradient(135deg, var(--colours-blue) 0%, var(--colours-blue-hint) 100%)" href="/posts">Posts</a></nav></header><main class="_app_content__3Z-Um"><article class="styles_post__v5a44"><section class="styles_categories__1BdoG"><a class=" styles_category__1Yob3" style="--category-background:linear-gradient(135deg, var(--colours-blue) 0%, var(--colours-blue-hint) 100%)" href="/categories/dev">dev</a></section><h1 class="styles_title__8NRYT">Making My Interactive Memoji</h1><h2 class="styles_description__1Ei_s">Interactive Memoji for the Web</h2><span class="styles_date__kx0TF">12/16/2020</span><span class="styles_body__hfQyP" id="HACK-making-my-memoji"><h2 id="memoji"><a href="#memoji">Memoji?</a></h2><p>Introduced with iOS 12, Memoji is Apple&#x27;s progression of the ubiquitous emoji. Set apart from its ancestors, Memoji are personalised caricatures, offering near infinite options for customisation.</p><noscript><style>[data-noscript="no"]{display: none!important}</style><img id="0.8348140904338743" src="/.assets/8225697f392f56e9e1b65e4cc963bc3c.webp" srcSet="/.assets/8225697f392f56e9e1b65e4cc963bc3c.webp 1x, /.assets/09d9af82603cfcf70d08ebba1a9259c0.webp 2x" width="1000" height="400" class="styles_loading__12PPP " loading="lazy" alt="Memoji" data-noscript="yes"/></noscript><img id="0.8348140904338743" src="" srcSet="" width="1000" height="400" class="styles_loading__12PPP " loading="lazy" alt="Memoji" data-noscript="no"/><p>Memoji&#x27;s good looks haven&#x27;t gone unrecognised, and many people use static, immobile Memoji as profile pictures, or as avatars on their website. I wanted to follow suit, but by opting for the static image, you really lose their sense of personality and liveliness. I didn&#x27;t want to settle for this, and chose to explore the possibility of embedding <strong>dynamic</strong> and <strong>interactive</strong> Memoji instead.</p><p>The fact that they&#x27;re fully rigged, and tightly integrated with iOS devices&#x27; built-in face tracking technology makes them really easy to create, and perfect for the majority who don&#x27;t possess the artistic aptitude to use Blender for such tasks. This can look really great<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> however, and should be the default first choice for anyone who&#x27;s able to.</p><p>They&#x27;re not without their flaws though; tightly integrated into Apple&#x27;s own ecosystem, Memoji and the mechanisms to create them (ie <code>AvatarKit</code>) are <strong>not</strong> exposed outside of iMessage and FaceTime. Apple has a history of limiting the use of their emojis<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>, and I imagine the restrictions on Memoji stem from the same corporate delusion. They&#x27;ll probably also tell you that using Memoji outside of their ecosystem is not allowed. Oops üñï.</p><p><strong>So, this is what we&#x27;ll be creating:</strong></p><section class="styles_memojiContainer__11udl"><canvas class="styles_memoji__3jDkc styles_memoji__35yAG styles_notReady__NraKJ" width="688" height="525"></canvas></section><h2 id="the-plan-of-attack"><a href="#the-plan-of-attack">The Plan of Attack</a></h2><blockquote><p><strong>N.B.</strong> A lot of this post didn&#x27;t initially work - so it&#x27;s just here to document the journey.</p><p>‚ú®. Skip ahead a bit if you&#x27;re only interested in what <em>did</em> work.</p></blockquote><p>Firstly let&#x27;s set out what we&#x27;re trying to achieve, then we&#x27;ll visit each of them in turn:</p><ol><li>Find a way of <strong>creating</strong> a Memoji</li><li>Get said Memoji <strong>into a useable format</strong></li><li>Insert Memoji into our <strong>website</strong></li><li>Allow users to <strong>interact</strong> with the Memoji</li></ol><h2 id="like-a-record-baby"><a href="#like-a-record-baby">Like a record baby‚Ä¶</a></h2><p>Creating the Memoji is probably the simplest of the steps, although there are a few things to note.</p><p>We&#x27;re going to to use iMessage to create out Memoji. Importantly we want to create a <em>video</em>, <strong>not</strong> a Memoji sticker.</p><noscript><style>[data-noscript="no"]{display: none!important}</style><img id="0.5964380049608711" src="/.assets/25a3cd56a2be982cda514ca79883ea57.webp" srcSet="/.assets/25a3cd56a2be982cda514ca79883ea57.webp 1x, /.assets/5cfe01c8ff3c6994222c6ad102087476.webp 2x" width="414" height="465" class="styles_loading__12PPP " loading="lazy" alt="iMessage" data-noscript="yes"/></noscript><img id="0.5964380049608711" src="" srcSet="" width="414" height="465" class="styles_loading__12PPP " loading="lazy" alt="iMessage" data-noscript="no"/><p>You want to record yourself rotating your head while keeping eye contact with the camera. The initial idea was to have the Memoji&#x27;s <strong>gaze</strong> following your mouse, but due to the limitations of our method, we can&#x27;t converge or diverge the eyes, nor face the head straight ahead. These issues ruin the effect if we go for a &#x27;following the mouse&#x27; type effect, so it&#x27;s better to go for a &#x27;tilting the head&#x27; type effect instead.</p><p>Have a play with the finished product, and compare that with your actual head:</p><p>Imagine you&#x27;re holding a hula hoop in front of you with both hands, and that it&#x27;s positioned so your face is inline with the centre. Now tilt your head (not your eyes) so that you&#x27;re looking at the top of the hoop. Follow the hoop around 360¬∞: no problem, we&#x27;re able to recreate this.</p><p>Now looking at the top of the hoop again, move straight down to the bottom of the hoop without following its curvature. And the same from left to right. Repeat the same with the finished Memoji above. Notice how in real life your head moves away from the outline of the imaginary hoop, <em>but</em> how in the Memoji the head doesn&#x27;t tilt to look <strong>within</strong> the hoop. This is the limitation we&#x27;re talking about.</p><p>It&#x27;s kinda okay though, your mouse isn&#x27;t normally over the Memoji, so it isn&#x27;t too much of an issue.</p><p>Anyway, recording the video! Mimic what you see in the finished article. Start the video with your head slightly tilting up, and slowly rotate it around 360¬∞.</p><noscript><style>[data-noscript="no"]{display: none!important}</style><img id="0.3960610816178416" src="/.assets/930f96cfa108a410442418563ee3cac7.webp" srcSet="/.assets/930f96cfa108a410442418563ee3cac7.webp 1x, /.assets/cb94ff261701a2b4872a0037bbb5e82d.webp 2x" width="842" height="108" class="styles_loading__12PPP " loading="lazy" alt="Taking the Video" data-noscript="yes"/></noscript><img id="0.3960610816178416" src="" srcSet="" width="842" height="108" class="styles_loading__12PPP " loading="lazy" alt="Taking the Video" data-noscript="no"/><p>You want to try and maintain a constant distance from the camera, as you want the first and last frame to be as close together as possible to hide the fact that the motion isn&#x27;t actually continuous. This took many attempts, and while the recording I settled on isn&#x27;t perfect, it&#x27;s good enough - and I&#x27;m not rerecording it for the 48<sup>th</sup> time üòÖ.</p><blockquote><p><strong>N.B.</strong> You&#x27;ll probably find that no matter how hard you try, the raw video will never loop due to the &#x27;reset&#x27; animation added at the end.</p><p>üîÅ Pause for a second at the end of the recording and then trim the video to achieve the loop. It&#x27;ll still take a few tries.</p></blockquote><h2 id="channel-your-inner-alpha"><a href="#channel-your-inner-alpha">Channel Your Inner Alpha!</a></h2><p>Apple doesn&#x27;t expose the Memoji in any sort of &#x27;raw&#x27; or 3D format. Instead, all we&#x27;ve got is a video. A <code>.mov</code>. Straight out of my iPhone 11, across iMessage, and into trusty <code>FFmpeg</code>, we can see some quite detailed information about the file.</p><pre class="language-txt"><code class="language-txt">Input #0, <span class="token highlight">‚Äãmov‚Äã</span>,mp4,m4a,3gp,3g2,mj2, from &#x27;./me-360t.mov&#x27;:
  Metadata:
    major_brand     : qt
    minor_version   : 0
    compatible_brands: qt
    creation_time   : 2020-11-24T18:30:38.000000Z
    <span class="token highlight">‚Äãcom.apple.quicktime.description: Memoji video, skin tone: Fairest Skin Tone 1; hairstyle: Maple, Short and straight updo with side parting; eyes: Blue;, facial hair: Flaxen, Shadow moustache and beard;‚Äã</span>
  Duration: 00:00:05.25, start: 0.000000, bitrate: 1992 kb/s
    Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 63 kb/s (default)
    Metadata:
      creation_time   : 2020-11-24T18:30:38.000000Z
      handler_name    : Core Media Audio
    Stream #0:1(und): Video: <span class="token highlight">‚Äãhevc‚Äã</span> (Main) (hvc1 / 0x31637668), <span class="token highlight">‚Äãyuv420p‚Äã</span>(tv, bt709), 640x480, 1753 kb/s, 57.56 fps, 60 tbr, 600 tbn, 600 tbc (default)
    Metadata:
      creation_time   : 2020-11-24T18:30:38.000000Z
      handler_name    : Core Media Video
      encoder         : HEVC

</code></pre><p>Firstly we can see that we&#x27;re dealing with a <code>.mov</code> container holding <code>HEVC</code> aka <code>H.265</code> encoded data. This is good news: a modern-ish format that supports alpha channels. Interestingly Apple also embeds plenty of metadata about the Memoji in the file. Useful for an accessibility description maybe üòØ!</p><pre class="language-txt"><code class="language-txt">com.apple.quicktime.description:
    Memoji video,
        skin tone: Fairest Skin Tone 1;
        hairstyle: Maple, Short and straight updo with side parting;
      eyes: Blue;
     ,facial hair: Flaxen, Shadow moustache and beard;
</code></pre><p>The file format supports alpha channels! Great! Would Apple be so good as to make sure of it? Opening the file in QuickTime we can see that there is indeed an alpha channel - and its being used üéâ!</p><noscript><style>[data-noscript="no"]{display: none!important}</style><img id="0.8239846449082346" src="/.assets/b98a4b7d7ec1fd17c8bc4073d58e130b.webp" srcSet="/.assets/b98a4b7d7ec1fd17c8bc4073d58e130b.webp 1x, /.assets/7e4768587a7152dc8324cf3a45bf449d.webp 2x" width="388" height="525" class="styles_loading__12PPP " loading="lazy" alt="QuickTime" data-noscript="yes"/></noscript><img id="0.8239846449082346" src="" srcSet="" width="388" height="525" class="styles_loading__12PPP " loading="lazy" alt="QuickTime" data-noscript="no"/><p>While the file has an alpha channel, I couldn&#x27;t get <code>FFmpeg</code> to acknowledge it. Looking in the output above we only see a pixel format of <code>yuv420p</code> listed. It seems that as of now, <code>FFmpeg</code>&#x27;s <code>libx265</code> codec doesn&#x27;t support <code>H.265</code> with <code>yuva420p</code> pixel data<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> - the key difference between <code>yuva420p</code> and <code>yuv420p</code> being the <code>a</code>, for alpha.</p><p>My initial idea was to use <code>FFmpeg</code> to transcode the <code>H.265</code> into <code>VP8</code>/<code>VP9</code> given its less than stellar cross compatibility<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>, then I&#x27;d have support for all major browsers using both formats combined<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>.</p><p>While I was able to convert the <code>.mov</code> to <code>.webm</code>, due to <code>FFmpeg</code>&#x27;s lack of <code>yuva420p</code> support I was loosing the alpha channel - which we&#x27;re trying to preserve üôÉ.</p><h2 id="changing-tact"><a href="#changing-tact">Changing Tact</a></h2><p>Rereading the <code>FFmpeg</code> output, I noticed the <code>handler_name</code> was <code>Core Media Video</code>. That&#x27;s a very &#x27;Apple&#x27; name, and indeed it&#x27;s the name of Apple&#x27;s shared macOS/iOS media framework. Given that QuickTime reports it, iOS generated it, and Preview shows it, I guessed there was a fair chance that tools I&#x27;d need to make use of the <code>yuva420p</code> alpha channel were already built in to macOS. If I could use these, then I could export the frames, and reencode them to <code>webm</code> using <code>FFmpeg</code>.</p><p>Again with Apple, this turned out to be an issue of exposing the means to the end user. QuickTime 7 <em>used</em> to offer the <code>export to image sequence</code> functionality<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>, but sometime between versions 7 and 10.6 the option was canned. QuickTime 7 isn&#x27;t supported after macOS Mojave<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup> üò¢. iMovie also lacks the capability. <strong>I can confirm that Final Cut Pro X does have it though!</strong></p><p>I don&#x27;t really want to buy FCP X, and while it&#x27;s only one click away on many-a-site üëÄ‚Ä¶it&#x27;s a bit overkill for such a small task.</p><h2 id="upvc"><a href="#upvc">uPVC</a></h2><p>Given that the <code>Core Media</code> framework is built in to macOS, I decided the path of least resistance was probably to just write a quick CLI tool to extract the frames using <code>AVFoundation</code> (which uses <code>Core Media</code> under the hood). And that&#x27;s exactly what I did:</p><figure class="code"><pre class="language-swift" metastring="Swift"><code class="language-swift" metastring="Swift"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token keyword">let</span> trackReaderOutput <span class="token operator">=</span> <span class="token function">AVAssetReaderTrackOutput</span><span class="token punctuation">(</span>track<span class="token punctuation">:</span> videoTrack<span class="token punctuation">,</span> outputSettings<span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token function">String</span><span class="token punctuation">(</span><span class="token constant">kCVPixelBufferPixelFormatTypeKey</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token function">NSNumber</span><span class="token punctuation">(</span>value<span class="token punctuation">:</span> kCVPixelFormatType_32BGRA<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token keyword">while</span> <span class="token keyword">let</span> sampleBuffer <span class="token operator">=</span> trackReaderOutput<span class="token punctuation">.</span><span class="token function">copyNextSampleBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">if</span> <span class="token keyword">let</span> imageBuffer <span class="token operator">=</span> <span class="token function">CMSampleBufferGetImageBuffer</span><span class="token punctuation">(</span>sampleBuffer<span class="token punctuation">)</span> <span class="token punctuation">{</span>
      <span class="token keyword">let</span> ciimage<span class="token punctuation">:</span> <span class="token builtin">CIImage</span> <span class="token operator">=</span> <span class="token function">CIImage</span><span class="token punctuation">(</span>cvPixelBuffer<span class="token punctuation">:</span> imageBuffer<span class="token punctuation">)</span>
      <span class="token keyword">if</span> <span class="token keyword">let</span> colorSpace <span class="token operator">=</span> <span class="token function">CGColorSpace</span><span class="token punctuation">(</span>name<span class="token punctuation">:</span> <span class="token builtin">CGColorSpace</span><span class="token punctuation">.</span>sRGB<span class="token punctuation">)</span> <span class="token punctuation">{</span>
          <span class="token keyword">let</span> format <span class="token operator">=</span> <span class="token builtin">CIFormat</span><span class="token punctuation">.</span><span class="token builtin">RGBA16</span> <span class="token comment">// 16-bit RGBA</span>
          <span class="token keyword">let</span> quality <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token comment">// 1.0 = lossless</span>

            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

          <span class="token keyword">try</span> context<span class="token punctuation">.</span><span class="token function">writePNGRepresentation</span><span class="token punctuation">(</span>of<span class="token punctuation">:</span> ciimage<span class="token punctuation">,</span> to<span class="token punctuation">:</span> outURL<span class="token punctuation">,</span> format<span class="token punctuation">:</span> format<span class="token punctuation">,</span> colorSpace<span class="token punctuation">:</span> colorSpace<span class="token punctuation">,</span> options<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token constant">kCGImageDestinationLossyCompressionQuality</span> <span class="token keyword">as</span> <span class="token builtin">CIImageRepresentationOption</span><span class="token punctuation">:</span> quality<span class="token punctuation">]</span><span class="token punctuation">)</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  frameIndex <span class="token operator">+</span><span class="token operator">=</span> <span class="token number">1</span>
<span class="token punctuation">}</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre><figcaption><a href="https://github.com/reubn/uPVC">https://github.com/reubn/uPVC</a></figcaption></figure><p>Written in <code>Swift</code>, it just iterates over the frames of a video file (which could theoretically be any video format supported by <code>AVFoundation</code>), and exports <code>16-bit RGBA</code> lossless <code>PNG</code>s. I&#x27;m not a <code>Swift</code> native, so the main logic was <del>copied</del> adapted üëå from Stack Overflow<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>. I also ended up adding the options to export every <code>n % x === 0</code>-th frame: for reasons I&#x27;ll come on to later üòè.</p><h2 id="return-of-the-ffmpeg"><a href="#return-of-the-ffmpeg">Return of the <code>FFmpeg</code>!</a></h2><p>Armed with our sequence of image files, we&#x27;re ready to merge them back into a video using <code>FFmpeg</code>.</p><pre class="language-bash" metastring="ZSH"><code class="language-bash" metastring="ZSH">ffmpeg<span class="token argument"> -framerate</span> <span class="token number">60</span><span class="token argument"> -i</span> ./frame-%0d.png<span class="token argument"> -c:v</span> libvpx-vp9<span class="token argument"> -pix_fmt</span> yuva420p<span class="token argument"> -speed</span> <span class="token number">0</span><span class="token argument"> -crf</span> <span class="token number">16</span><span class="token argument"> -an</span> ./me.webm
</code></pre><p><code>FFmpeg</code> does support <code>yuva420p</code> with <code>webm</code>, so we&#x27;re good, and the file size isn&#x27;t terrible - at around <code>1MB</code>.</p><h2 id="rough-as-a-bears-arse"><a href="#rough-as-a-bears-arse">Rough as a Bear&#x27;s Arse</a></h2><p>All that&#x27;s left now is to embed the video, and hookup <code>mousemove</code> event handlers to scrub through the video. Surely.</p><p>In Safari this works fine - buttery smooth - Chrome however really chokes on the video. I tried lowering the quality via the <code class="language-bash">-crf</code> parameter in <code>FFmpeg</code>, but no matter how low the quality, the FPS, or how much event handler throttling I applied, Chrome still jittered and stalled its way through the animation. The same applied for <code>VP8</code>.</p><noscript><style>[data-noscript="no"]{display: none!important}</style><video width="677" height="370" class="styles_loading__1HQrJ " controls="" muted="" autoplay="" loop="" data-noscript="yes"><source src="/.assets/d05f13b4b1aba3a83466bb937920bfa0.webm" type="video/webm"/><source src="/.assets/bf0c0fbce229c1188b5907755afd89d9.mp4" type="video/mp4"/></video></noscript><video width="677" height="370" class="styles_loading__1HQrJ " controls="" muted="" autoplay="" loop="" data-noscript="no"><source type="video/webm"/><source type="video/mp4"/></video><p>Given that Safari has no trouble I wonder if this is an issue of the <code>HTML5</code> <code>video</code> element&#x27;s scrubbing not being optimised for animation via <code class="language-js"><span class="token punctuation">.</span><span class="token property-access">currentTime</span></code> in <code>Blink</code>-based browsers. I also tried this in combination with <code class="language-js"><span class="token dom variable">window</span><span class="token punctuation">.</span><span class="token property-access">requestAnimationFrame</span></code> to no avail.</p><p>Another difference between the browsers was that Safari was consuming the <code>.mov</code> file, while Chrome was using the <code>.webm</code> file. They both have similar file sizes, but very different compression algorithms. Regardless, most compressed video cannot be played back in reverse, or be seeked<sup>(sic)</sup> at realtime speeds.</p><p>As an analogy, most video compression works a bit like a zip (no pun intended):</p><ul><li>ü¶∑ Teeth of the zip represent video frames</li><li>üëç You can unzip your coat in one direction, and it works fine.</li><li>üëé You can&#x27;t unzip your coat in reverse - or skip bits out - leaving them zipped together.</li><li>ü§ê Picking a random point on the zip, all preceding zip must be undone in order to reach it.</li></ul><p>Keeping it simple - in video you have <code>I frames</code>, and <code>P frames</code><sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>.<code>I frames</code> are like the frames that we extracted using <code>uPVC</code> - whole images in their own right. <code>P frames</code> are not bona fide images. Instead they contain references to previous <code>I</code> and <code>P frames</code>, specifying elements they have in common to save space, and how they differ.</p><ul><li>To decode an <code>I frame</code> you just need that frame üëå.</li><li>To decode a <code>P frame</code> however you also need all preceding frames that it references, and all preceding frames that all those frames reference ‚Ä¶ all the way back to the closest <code>I frame</code><sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup>.</li></ul><p>Scrubbing through the video (seeking in response to mouse movements) would require the decoder to jump around &#x27;the zip&#x27; and necessitate &#x27;unzipping&#x27; a lot of previous (which is <strong>ahead</strong> if playing in reverse) frames / footage. Perhaps this aspect of video encoding is where some of the issues arose.</p><p>This doesn&#x27;t fully explain the problems as there wasn&#x27;t such an issue with <code>H.265</code> in Safari - but it most probably contributed. Perhaps Chrome didn&#x27;t buffer the uncompressed video into memory, while Safari did ü§∑‚Äç‚ôÄÔ∏è.</p><h2 id="abort-abort"><a href="#abort-abort">Abort, Abort</a></h2><p>Given that the video method wasn&#x27;t going to work, I had to think of an alternative. If we run with the idea of video simply being a series of frames, it&#x27;s not a crazy idea to skip the whole &#x27;video&#x27; encoding malarkey and just deal with frames directly.</p><p>We&#x27;ve already done the hard work of extracting the frames, and have a folder of <code>.png</code> files.</p><p>I wrote a small <code>React</code> component to listen for global <code>mousemove</code> event, and display the corresponding frame. This works great, and surprisingly is a lot smoother than the video! I&#x27;m not going to go over the ins and outs of it here, but I will point out a few things of note.</p><p>Firstly the interactivity is a bonus, and in order to not get in the way of more pressing resources, we load the frames via a <code>useEffect</code> hook. We load a sensible <code>defaultFrame</code> first and that serves as a placeholder until the rest of them have loaded. We listen for the <code>.decode()</code> <code>Promise</code> on the <code>defaultFrame</code> <code>Image</code> to signal that it&#x27;s loaded and then animate it in.</p><p>Downloading lots of (smallish) image files isn&#x27;t a problem for people on fast internet connections, but consideration has to be taken for those on slower connections. The <code>React</code> component allows about <code>5sec</code> for the images to download before cancelling all but the <code>defaultFrame</code> request. This allows any other more important content to come through faster, and gracefully falls back to a static Memoji.</p><p>At this point we still don&#x27;t have interactivity. That only occurs once all the frames have downloaded - to stop jitter in the first few moments. In testing it was quite common to stop moving the mouse <em>just</em> before the point at which the frames became available. When the user then resumes mouse movement there&#x27;s a jump between the <code>defaultFrame</code> placeholder and the required frame.</p><p>To overcome this, we listen for the user&#x27;s mouse movement even before the frames have downloaded. There isn&#x27;t the option to directly query the user agent for the mouse position, so we rely on recording these preemptive events to get it - and then smoothly transition to the corresponding frame. It&#x27;s better to transition when the mouse isn&#x27;t moving than when it is.</p><p>This works fine on traditional mouse and keyboard devices, but - for obvious reasons - works a little differently on touchscreens. We listen for the <code>onTouch</code> counterparts, but also animate the Memoji by default - as to not rely on the user touching the Memoji. Once they have, touch interaction takes over. As a next step I&#x27;d like to overlay a call-to-action to highlight this to touch users, and make the animation a little more natural.</p><h2 id="a-little-on-the-large-side"><a href="#a-little-on-the-large-side">A Little on the Large Side</a></h2><p>Video compression may have hindered our dreams of smooth scrubbing, but it&#x27;s used for a reason. The 315 frames only took up around <code>1MB</code>. Extracted in their raw <code>.png</code> form they occupy a staggering <code>65.4MB</code>. Ouch üò≥. Admittedly <code>.png</code> probably isn&#x27;t the most efficient format for storing high fidelity graphics, but it is lossless, and does support alpha channels: fine for an intermediate format.</p><p>Firstly I saved <code>2/3</code> off the total size by only keeping every 3<sup>rd</sup> frame. While perhaps counterintuitive, this didn&#x27;t affect the smoothness of the final result in any perceivable way.</p><p><code>ImageOptim</code> managed to reduce the size down further by <code>47%</code>. But that&#x27;s still not anywhere near where we need it to be.</p><p>I settled on <code>.webp</code>. Support is good<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup>, and I set about converting the <code>.png</code> files. The <code>cwebp</code> tool<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup> is the way to go, and <code>.webp</code> itself has a multitude of options to when encoding the image.</p><pre class="language-bash" metastring="ZSH"><code class="language-bash" metastring="ZSH"><span class="token keyword">for</span> <span class="token for-or-select variable">file</span> <span class="token keyword">in</span> *.png<span class="token punctuation">;</span> <span class="token keyword">do</span> cwebp<span class="token argument"> -lossless</span><span class="token argument"> -mt</span> <span class="token string">&quot;./<span class="token variable">$file</span>&quot;</span><span class="token argument"> -o</span> <span class="token string">&quot;./<span class="token variable">${file<span class="token operator">/</span><span class="token operator">/</span>\.png<span class="token operator">/</span>.webp}</span>&quot;</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre><p> It&#x27;s a really neat format and supports both <code>lossless</code>, and <code>lossy</code> compression. It even has a middle ground <code>near_lossless</code> option. Even with <code>lossless</code> compression, it managed to reduce the total burden by a massive <code>93%</code> to only <code>4.6MB</code>! While really impressive, this is still a bit hefty, as I was hoping to dip into the <code>KB</code> range.</p><p> After going through many permutations, I settled on a <code>lossy</code> quality of <code>80</code>. Going lower reduced the image quality, but didn&#x27;t justify the cost, giving only negligible improvements in file size. I also used <code>cwebp</code>&#x27;s builtin crop function to try and shave off every last byte.</p><pre class="language-bash" metastring="ZSH"><code class="language-bash" metastring="ZSH"><span class="token keyword">for</span> <span class="token for-or-select variable">file</span> <span class="token keyword">in</span> *.png<span class="token punctuation">;</span> <span class="token keyword">do</span> cwebp<span class="token argument"> -q</span> <span class="token number">80</span><span class="token argument"> -mt</span><span class="token argument"> -crop</span> <span class="token number">100</span> <span class="token number">26</span> <span class="token number">459</span> <span class="token number">350</span> <span class="token string">&quot;./<span class="token variable">$file</span>&quot;</span><span class="token argument"> -o</span> <span class="token string">&quot;./<span class="token variable">${file<span class="token operator">/</span><span class="token operator">/</span>\.png<span class="token operator">/</span>.webp}</span>&quot;</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre><p>After running this, and cooking an egg on my CPU üç≥, I was left with a total size of <code>868KB</code>. Pretty fucking good üëå. A combined saving of just under <code>99%</code>!</p><p><code>.webp</code> itself is based on the <code>VP8</code> video codec that we wanted to use initially. Given this, I guess the savings aren&#x27;t that surprising, and basically means that we&#x27;ve come full circle, just without the limitations of video compression.</p><p>There are some avenues to explore to try and reduce the total file size some more. <code>.webp</code> supports animation, but I don&#x27;t think the ability to scrub through animated images is exposed in the web environment at the present time, and it may well suffer from the same issues as <code>VP8</code>/<code>VP9</code>. Maybe serving the frames as an animated <code>.webp</code> file and then decoding them to canvas is an option - but I am yet to explore the possibility.</p><p>A simpler option might be to combine the frames into a sprite sheet.</p><pre class="language-bash" metastring="ZSH"><code class="language-bash" metastring="ZSH">montage ./*.webp<span class="token argument"> -define</span> webp:lossless<span class="token operator">=</span>true<span class="token argument"> -geometry</span> <span class="token string">&#x27;1x1+0+0&lt;&#x27;</span><span class="token argument"> -tile</span> 10x<span class="token argument"> -background</span> none ./output.webp

cwebp<span class="token argument"> -q</span> <span class="token number">80</span> ./output.webp<span class="token argument"> -o</span> ./compressed.webp
</code></pre><p>I&#x27;ve had a play using <code>ImageMagick</code>&#x27;s <code>montage</code>, and I&#x27;ve only been able to reduce the total size to about <code>715KB</code> after <code>lossy</code> compression. I still need to test if this meagre compression benefit outweighs the speed benefits that we achieve by serving individual frames concurrently over <code>HTTP2</code>.</p><noscript><style>[data-noscript="no"]{display: none!important}</style><img id="0.9186025097682864" src="/.assets/bc144f17774402c6846b790407e433b8.webp" srcSet="/.assets/bc144f17774402c6846b790407e433b8.webp 1x" width="4590" height="3850" class="styles_loading__12PPP " loading="lazy" alt="Sprites" data-noscript="yes"/></noscript><img id="0.9186025097682864" src="" srcSet="" width="4590" height="3850" class="styles_loading__12PPP " loading="lazy" alt="Sprites" data-noscript="no"/><h2 id="are-we-nearly-there-yet"><a href="#are-we-nearly-there-yet">Are we nearly there yet?</a></h2><p>While what we have works, it still has a number of caveats, chiefly the <code>868KB</code> file size burden, and the limited axises the Memoji is able to move in. I&#x27;d still like to reduce the former, and to summarise, the avenues I&#x27;m currently exploring are:</p><ul><li>Animated <code>.webp</code> frames decoded to <code>canvas</code></li><li>Sprite sheets</li><li>Load reduced quality frames for movement, and download higher quality once movement has stopped</li><li>Only load frames that surround cursor&#x27;s starting position</li><li><code>.avif</code> once support is good enough</li></ul><p>The latter is more difficult, and would require a completely new approach. While the <code>AvatarKit</code> framework isn&#x27;t officially exposed, people have managed to use it on side loaded apps<sup id="fnref-13"><a href="#fn-13" class="footnote-ref">13</a></sup>. The 3D models are also stored at a known location<sup id="fnref-14"><a href="#fn-14" class="footnote-ref">14</a></sup>, extracting them shouldn&#x27;t be difficult, but I have very little knowledge of how to composite the features into &#x27;me&#x27;, nor do I know how animating them would work. If this were to be possible, then it would solve the current limitations on movement - <em>and</em> - via <code>WebGL</code> the issues with file size.</p><div class="footnotes"><hr/><ol><li id="fn-1" ref-number="1"><a href="https://www.joshwcomeau.com/">https://www.joshwcomeau.com/</a><a href="#fnref-1" class="footnote-backref">‚Ü©</a></li><li id="fn-2" ref-number="2"><a href="https://twitter.com/Sam0711er/status/959535639174746112">https://twitter.com/Sam0711er/status/959535639174746112</a><a href="#fnref-2" class="footnote-backref">‚Ü©</a></li><li id="fn-3" ref-number="3"><a href="https://trac.ffmpeg.org/ticket/7965">https://trac.ffmpeg.org/ticket/7965</a><a href="#fnref-3" class="footnote-backref">‚Ü©</a></li><li id="fn-4" ref-number="4"><a href="https://caniuse.com/hevc">https://caniuse.com/hevc</a><a href="#fnref-4" class="footnote-backref">‚Ü©</a></li><li id="fn-5" ref-number="5"><a href="https://caniuse.com/webm">https://caniuse.com/webm</a><a href="#fnref-5" class="footnote-backref">‚Ü©</a></li><li id="fn-6" ref-number="6"><a href="https://video.stackexchange.com/questions/4033/exporting-mov-vr-to-multiple-still-frames">https://video.stackexchange.com/questions/4033/exporting-mov-vr-to-multiple-still-frames</a><a href="#fnref-6" class="footnote-backref">‚Ü©</a></li><li id="fn-7" ref-number="7"><a href="https://support.apple.com/kb/DL923?locale=en_GB">https://support.apple.com/kb/DL923?locale=en_GB</a><a href="#fnref-7" class="footnote-backref">‚Ü©</a></li><li id="fn-8" ref-number="8"><a href="https://stackoverflow.com/questions/39570745/accesing-individual-frames-using-av-player/39573702#39573702">https://stackoverflow.com/questions/39570745/accesing-individual-frames-using-av-player/39573702#39573702</a><a href="#fnref-8" class="footnote-backref">‚Ü©</a></li><li id="fn-9" ref-number="9"><a href="https://en.m.wikipedia.org/wiki/Video_compression_picture_types">https://en.m.wikipedia.org/wiki/Video_compression_picture_types</a><a href="#fnref-9" class="footnote-backref">‚Ü©</a></li><li id="fn-10" ref-number="10"><a href="https://github.com/google/ExoPlayer/issues/2191#issuecomment-266783472">https://github.com/google/ExoPlayer/issues/2191#issuecomment-266783472</a><a href="#fnref-10" class="footnote-backref">‚Ü©</a></li><li id="fn-11" ref-number="11"><a href="https://caniuse.com/webp">https://caniuse.com/webp</a><a href="#fnref-11" class="footnote-backref">‚Ü©</a></li><li id="fn-12" ref-number="12"><a href="https://developers.google.com/speed/webp/docs/cwebp">https://developers.google.com/speed/webp/docs/cwebp</a><a href="#fnref-12" class="footnote-backref">‚Ü©</a></li><li id="fn-13" ref-number="13"><a href="https://github.com/simonbs/SBSAnimoji">https://github.com/simonbs/SBSAnimoji</a><a href="#fnref-13" class="footnote-backref">‚Ü©</a></li><li id="fn-14" ref-number="14"><a href="https://www.reddit.com/r/jailbreak/comments/j2rxd5/question_is_there_a_way_to_ripexport/">https://www.reddit.com/r/jailbreak/comments/j2rxd5/question_is_there_a_way_to_ripexport/</a><a href="#fnref-14" class="footnote-backref">‚Ü©</a></li></ol></div></span></article><script>window.__HACK_GLOBAL = {...(window.__HACK_GLOBAL || {}), ["making-my-memoji"]: document.getElementById('HACK-making-my-memoji').innerHTML}</script></main><footer class="styles_footer__3X29e"><p class="styles_icons__9pmlP"><a href="//unsplash.com/@re" class="styles_unsplash__2dXtF" aria-label="unsplash"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><path d="M23 19a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h4l2-3h6l2 3h4a2 2 0 0 1 2 2z"></path><circle cx="12" cy="13" r="4"></circle></svg></a><a href="//github.com/reubn" class="styles_github__2s_Vd" aria-label="github"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a><a href="mailto:click.to.reveal@email.com" class="styles_mail__GwmOI" aria-label="email"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></a><a href="//twitter.com/reubn_" class="styles_twitter__3GQFb" aria-label="twitter"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg></a><a href="//instagram.com/reubn" class="styles_instagram__3QWSe" aria-label="instagram"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 132 132" width="0" height="0"><defs><linearGradient id="instagram" gradientTransform="rotate(45)"><stop offset="0" stop-color="#fd5"></stop><stop offset=".1" stop-color="#fd5"></stop><stop offset=".5" stop-color="#ff543e"></stop><stop offset="1" stop-color="#c837ab"></stop></linearGradient></defs></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" y1="6.5" x2="17.51" y2="6.5"></line></svg></a></p><a href="//github.com/reubn/reuben.science" class="styles_code__3w15N" aria-label="source code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="styles_icon__1r7NH"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg></a></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"making-my-memoji","metadata":{"title":"Making My Interactive Memoji","description":"Interactive Memoji for the Web","emoji":"üë¶üèª","date":"2020-12-16T00:00:00.000Z","category":["dev"],"image":{"size":{"width":1000,"height":400},"src":"/.assets/8225697f392f56e9e1b65e4cc963bc3c.webp","srcSet":"/.assets/8225697f392f56e9e1b65e4cc963bc3c.webp 1x, /.assets/09d9af82603cfcf70d08ebba1a9259c0.webp 2x"},"readingTime":"19 min"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"making-my-memoji"},"buildId":"P5jkL99xrm58LL1_XtOCf","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-49d513b8e73258edd797.js"></script><script src="/_next/static/chunks/main-6977ff558a8644f11edb.js" async=""></script><script src="/_next/static/chunks/webpack-7ec0fa63faecf3764afe.js" async=""></script><script src="/_next/static/chunks/framework.28e15cca58e295c8d62e.js" async=""></script><script src="/_next/static/chunks/commons.a7eaae1b56f101b95777.js" async=""></script><script src="/_next/static/chunks/75dcd2c344866c2e9d678f03e7caf5a73214c3a2.374f04e66024ab278f54.js" async=""></script><script src="/_next/static/chunks/pages/_app-337c0d7807df1f95e2d3.js" async=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-1fb1690444c44a1b1d3d.js" async=""></script><script src="/_next/static/P5jkL99xrm58LL1_XtOCf/_buildManifest.js" async=""></script><script src="/_next/static/P5jkL99xrm58LL1_XtOCf/_ssgManifest.js" async=""></script></body></html>